diff --git a/.pylintrc b/.pylintrc
index 49f1508..de49054 100644
--- a/.pylintrc
+++ b/.pylintrc
@@ -360,10 +360,10 @@ max-statements=50
 max-parents=7
 
 # Maximum number of attributes for a class (see R0902).
-max-attributes=7
+max-attributes=8
 
 # Minimum number of public methods for a class (see R0903).
-min-public-methods=2
+min-public-methods=1
 
 # Maximum number of public methods for a class (see R0904).
 max-public-methods=20
diff --git a/netineti/finder.py b/netineti/finder.py
index 661d0db..32aefcb 100644
--- a/netineti/finder.py
+++ b/netineti/finder.py
@@ -8,9 +8,9 @@ Output : A list of scientific names
 import os
 import re
 import nltk
-from .helper import *
+import netineti.helper as helper
 
-class NetiNeti():
+class NetiNeti(object):
     """Uses the trained NetiNetiTrainer model and searches through text
     to find names.
 
@@ -29,7 +29,7 @@ class NetiNeti():
         """
         self._black_dict = {}
         black_list = open(os.path.dirname(os.path.realpath(__file__)) + "/"  +
-                     black_list_file)
+                          black_list_file)
         for line in black_list:
             self._black_dict[line.rstrip()] = 1
         self._model_object = model_object
@@ -73,7 +73,7 @@ class NetiNeti():
         tokens -- list with all tokens from the searched document
 
         """
-        self._index_dict = create_index(tokens)
+        self._index_dict = helper.create_index(tokens)
         token_string = " ".join(tokens)
 
         if len(tokens) == 2:
@@ -82,10 +82,10 @@ class NetiNeti():
                 self._names_list.append(token_string)
         elif len(tokens) == 1:
             if (len(tokens[0]) > 2
-                and tokens[0][0].isupper()
-                and tokens[0].isalpha()
-                and self._is_not_in_black_list(tokens[0])
-                and self._is_a_name(tokens[0], tokens, 0, 0)):
+                    and tokens[0][0].isupper()
+                    and tokens[0].isalpha()
+                    and self._is_not_in_black_list(tokens[0])
+                    and self._is_a_name(tokens[0], tokens, 0, 0)):
                 self._names_list.append(tokens[0])
         else:
             trigrams = list(nltk.trigrams(tokens))
@@ -104,12 +104,12 @@ class NetiNeti():
         for offset in self._offsets_list:
             name = self._text[offset[0]:offset[1]]
             parts = name.split(" ")
-            if(parts[0][0] + parts[0][-1] == "()"):
+            if parts[0][0] + parts[0][-1] == "()":
                 offset1 = offset[0]
-                offset2 = offset[1] + right_strip(name)[1]
+                offset2 = offset[1] + helper.right_strip(name)[1]
             else:
-                offset1 = offset[0] + left_strip(name)[1]
-                offset2 = offset[1] + right_strip(name)[1]
+                offset1 = offset[0] + helper.left_strip(name)[1]
+                offset2 = offset[1] + helper.right_strip(name)[1]
             name = self._text[offset1:offset2]
             names_verbatim.append(name)
             offsets.append((offset1, offset2))
@@ -126,37 +126,44 @@ class NetiNeti():
         """
         for word1_orig, word2_orig, word3_orig in trigrams:
             self._count += 1
-            word1, word2, word3 = clean_token(word1_orig.strip(),
-                word2_orig.strip(), word3_orig)
-            bigram = remove_trailing_period(word1 + " " + word2)
-            trigram = remove_trailing_period(word1 + " " + word2 + " " + word3)
-            if(self._is_like_trinomial(word1, word2, word3)):
-                if(self._is_a_name(trigram, tokens, self._count, 2)):
+            word1, word2, word3 = helper.clean_token(word1_orig.strip(),
+                                                     word2_orig.strip(),
+                                                     word3_orig)
+            bigram = helper.remove_trailing_period(word1 + " " + word2)
+            trigram = helper.remove_trailing_period(word1 + " " + word2 +
+                                                    " " + word3)
+            if self._is_like_trinomial(word1, word2, word3):
+                if self._is_a_name(trigram, tokens, self._count, 2):
                     start, end = self._get_offsets(word1_orig, word2_orig,
-                        word3_orig)
+                                                   word3_orig)
                     self._offsets_list.append((start, end))
                     self._prepare_name(word1, word2, word3)
-            elif(self._is_like_binomial(word1, word2)):
-                if(self._is_a_name(bigram, tokens, self._count, 1)):
+            elif self._is_like_binomial(word1, word2):
+                if self._is_a_name(bigram, tokens, self._count, 1):
                     start, end = self._get_offsets(word1_orig, word2_orig)
                     self._offsets_list.append((start, end))
                     self._prepare_name(word1, word2, "")
-            elif(self._is_like_uninomial(word1)):
-                if self._is_a_name(re.sub("\.", ". ",
-                    remove_trailing_period(word1)), tokens, self._count, 0):
+            elif self._is_like_uninomial(word1):
+                if self._is_a_name(
+                        re.sub(r"\.", ". ",
+                               helper.remove_trailing_period(word1)),
+                        tokens, self._count, 0):
                     start, end = self._get_offsets(word1_orig)
                     self._offsets_list.append((start, end))
-                    self._names_list.append(remove_trailing_period(word1))
-                elif(has_uninomial_ending(word1)):
+                    self._names_list.append(
+                        helper.remove_trailing_period(word1))
+                elif helper.has_uninomial_ending(word1):
                     start, end = self._get_offsets(word1_orig)
                     self._offsets_list.append((start, end))
-                    self._names_list.append(remove_trailing_period(word1))
-            elif(has_uninomial_ending(word1)):
-                if(self._is_not_in_black_list(word1) and word1[0].isupper()
-                    and remove_trailing_period(word1).isalpha()):
+                    self._names_list.append(
+                        helper.remove_trailing_period(word1))
+            elif helper.has_uninomial_ending(word1):
+                if (self._is_not_in_black_list(word1) and word1[0].isupper()
+                        and helper.remove_trailing_period(word1).isalpha()):
                     start, end = self._get_offsets(word1_orig)
                     self._offsets_list.append((start, end))
-                    self._names_list.append(remove_trailing_period(word1))
+                    self._names_list.append(
+                        helper.remove_trailing_period(word1))
 
     def _check_last_bigram_unigram(self, trigram, tokens):
         """Returns None
@@ -169,8 +176,9 @@ class NetiNeti():
 
         """
 
-        bigram = remove_trailing_period(trigram[-2] + " " + trigram[-1])
-        unigram = re.sub("\. ", " ", remove_trailing_period(trigram[-2]))
+        bigram = helper.remove_trailing_period(trigram[-2] + " " + trigram[-1])
+        unigram = re.sub(r"\. ", " ",
+                         helper.remove_trailing_period(trigram[-2]))
         if self._is_like_binomial(trigram[-2], trigram[-1]):
             if self._is_a_name(bigram, tokens, self._count + 1, 1):
                 self._prepare_name(trigram[-2], trigram[-1], "")
@@ -186,17 +194,19 @@ class NetiNeti():
         word -- a word to check as a ponential uninomial
 
         """
-        #TODO: This method currently only allows uninomials of size larger
+        # This method currently only allows uninomials of size larger
         # than 5, however there are uninomials which are 2 characters in size.
 
         is_like_uninomial = (len(word) > 5
-            and word[0].isupper()
-            and word[1:].islower() and
-            (remove_trailing_period(word).isalpha()
-                or (word[0].isupper() and word[1] == "."
-                    and word[2].islower()
-                    and remove_trailing_period(word[2:]).isalpha()))
-            and self._is_not_in_black_list(word))
+                             and word[0].isupper()
+                             and word[1:].islower()
+                             and (helper.remove_trailing_period(word).isalpha()
+                                  or (word[0].isupper() and word[1] == "."
+                                      and word[2].islower()
+                                      and helper.
+                                      remove_trailing_period(word[2:]).
+                                      isalpha()))
+                             and self._is_not_in_black_list(word))
         return is_like_uninomial
 
     def _is_like_binomial(self, first_word, second_word):
@@ -211,11 +221,12 @@ class NetiNeti():
         if len(first_word) > 1 and len(second_word) > 1:
             is_abbr_word = (first_word[1] == '.' and len(first_word) == 2)
             is_a_candidate = (first_word[0].isupper()
-                and second_word.islower()
-                and ((first_word[1:].islower()
-                    and first_word.isalpha()) or is_abbr_word)
-                and (remove_trailing_period(second_word).isalpha()
-                or '-' in second_word))
+                              and second_word.islower()
+                              and ((first_word[1:].islower()
+                                    and first_word.isalpha()) or is_abbr_word)
+                              and (helper.
+                                   remove_trailing_period(second_word).isalpha()
+                                   or '-' in second_word))
             return (is_a_candidate
                     and self._is_not_in_black_list(first_word)
                     and self._is_not_in_black_list(second_word))
@@ -234,28 +245,30 @@ class NetiNeti():
         """
         if len(first_word) > 1 and len(second_word) > 1 and len(third_word) > 1:
             third_word_ok = (third_word.islower()
-                and remove_trailing_period(third_word).isalpha())
+                             and helper.remove_trailing_period(third_word).
+                             isalpha())
 
             if second_word[0] + second_word[-1] == "()":
                 second_word_ok = (second_word[1].isupper()
-                    and ((second_word[2] == "." and len(second_word) == 4)
-                    or second_word[2:-1].islower()
-                    and second_word[2:-1].isalpha())
-                    and second_word[-1] != ".")
+                                  and ((second_word[2] == "."
+                                        and len(second_word) == 4)
+                                       or second_word[2:-1].islower()
+                                       and second_word[2:-1].isalpha())
+                                  and second_word[-1] != ".")
                 return (second_word_ok and third_word_ok
                         and self._is_not_in_black_list(third_word)
                         and (first_word[0].isupper()
-                        and ((first_word[1] == "."
-                            and len(first_word) == 2)
-                            or first_word[1:].islower()
-                            and first_word.isalpha())))
+                             and ((first_word[1] == "."
+                                   and len(first_word) == 2)
+                                  or first_word[1:].islower()
+                                  and first_word.isalpha())))
             else:
                 return (third_word_ok
                         and self._is_like_binomial(first_word, second_word)
                         and self._is_not_in_black_list(third_word))
         elif (len(first_word) > 1
-                and len(second_word) == 0
-                and len(third_word) > 1):
+              and len(second_word) == 0
+              and len(third_word) > 1):
             return self._is_like_binomial(first_word, third_word)
         else:
             return False
@@ -268,10 +281,10 @@ class NetiNeti():
         word -- a token, first element of a trigram
 
         """
-        word = remove_trailing_period(word)
+        word = helper.remove_trailing_period(word)
         word_parts = word.split("-")
         res = [self._black_dict.has_key(part) for part in word_parts]
-        return (True not in res and not self._black_dict.has_key(word.lower()))
+        return True not in res and not self._black_dict.has_key(word.lower())
 
     def _is_a_name(self, token, context, index, span):
         """Returns a boolean
@@ -285,7 +298,7 @@ class NetiNeti():
 
         """
         features = self._model_object.taxon_features(token, context,
-            index, span)
+                                                     index, span)
         return self._model_object.get_model().classify(features) == 'taxon'
 
     def _prepare_name(self, word1, word2, word3):
@@ -298,20 +311,19 @@ class NetiNeti():
         word3 -- the third word of a name
 
         """
-        if(word2 == ""):
-            name = remove_trailing_period((word1 + " " + word3).strip())
+        if word2 == "":
+            name = helper.remove_trailing_period((word1 + " " + word3).strip())
         else:
-            name = remove_trailing_period((word1 + " " + word2
-                                           + " " + word3).strip())
-        if(name[1] == "." and name[2] == " "):
+            name = helper.remove_trailing_period((word1 + " " + word2 +
+                                                  " " + word3).strip())
+        if name[1] == "." and name[2] == " ":
             self._names_list.append(name)
         else:
             self._names_list.append(name)
-            self._names_dict[remove_trailing_period((word1[0] + ". "
-                + word2 + " " + word3).strip())] = word1[1:]
+            self._names_dict[helper.remove_trailing_period(
+                (word1[0] + ". " + word2 + " " + word3).strip())] = word1[1:]
 
-
-    def _get_offsets(self, word1, word2 = '', word3 = ''):
+    def _get_offsets(self, word1, word2='', word3=''):
         """Returns word1 tuple with start and end positions of
         word1 found scientific name.
 
@@ -324,6 +336,3 @@ class NetiNeti():
         name = name.strip()
         return (self._index_dict[self._count],
                 self._index_dict[self._count] + len(name))
-
-if __name__ == '__main__':
-    print "NETI..NETI\n"
diff --git a/netineti/test/finder_tests.py b/netineti/test/finder_tests.py
index 85a8d4d..911fbef 100644
--- a/netineti/test/finder_tests.py
+++ b/netineti/test/finder_tests.py
@@ -1,10 +1,16 @@
 # -*- coding: utf-8 -*-
 """Test finder module"""
-import logging
+import os
+import pickle
 import unittest
 from netineti.finder import NetiNeti
 from netineti.trainer import NetiNetiTrainer
 
+USE_DUMP = True #set to True if you are not fixing algorithms (saves time)
+
+PATH = os.path.dirname(os.path.realpath(__file__))
+TRAINER_DUMP = PATH + '/../data/netineti_trainer_dump'
+
 class TestFinder(unittest.TestCase):
     """Test helper functions"""
 
@@ -13,9 +19,12 @@ class TestFinder(unittest.TestCase):
         """ Sets trainer once per all tests.
 
         Run `nosetests` -s to see print output """
-        print ("Running trainer")
+        print "Running trainer"
         super(TestFinder, cls).setUpClass()
-        cls.nt = NetiNetiTrainer()
+        if USE_DUMP:
+            cls.nt = pickle.load(open(TRAINER_DUMP, 'rb'))
+        else:
+            cls.nt = NetiNetiTrainer()
         cls.nn = NetiNeti(TestFinder.nt)
 
     def test_uninomial(self):
@@ -23,10 +32,8 @@ class TestFinder(unittest.TestCase):
         res = TestFinder.nn.find_names("Poaceae are not spiders!")
         self.assertEqual(res, ('Poaceae', ['Poaceae'], [(0, 7)]))
 
-    def test_binomial(self):
-        """ Returns one found name from a text. The test shows a known
-        problem where empty space needs to be added in the end
-        for a name to be recognized """
+    def test_binomial_exact_match(self):
+        """ Returns one found name from a text containing only the name """
         res = TestFinder.nn.find_names("Pardosa moesta ")
         self.assertEqual(res, ('Pardosa moesta', ['Pardosa moesta'],
                                [(0, 14)]))
@@ -36,16 +43,18 @@ class TestFinder(unittest.TestCase):
         res = TestFinder.nn.find_names("Nothing that looks like a name")
         self.assertEqual(res, ("", [], []))
 
+    def test_black_dict(self):
+        """ Discards names from black list """
+
     def test_multiple_names(self):
         """ Returns multiple names """
         res = TestFinder.nn.find_names("""
         Homo sapiens ate Pardosa moesta for breakfast, then
         Parus major ate Homo sapiens for lunch """)
-        self.assertEqual(res,  ('Homo sapiens\nPardosa moesta\nParus major',
-                                ['Homo sapiens', 'Pardosa moesta',
-                                 'Parus major', 'Homo sapiens'],
-                                [(9, 21), (26, 40), (69, 80), (85, 97)]))
-
+        self.assertEqual(res, ('Homo sapiens\nPardosa moesta\nParus major',
+                               ['Homo sapiens', 'Pardosa moesta',
+                                'Parus major', 'Homo sapiens'],
+                               [(9, 21), (26, 40), (69, 80), (85, 97)]))
 
     def test_commas_between_words(self):
         """ Commas after 1st or 2nd word means stop looking... """
diff --git a/netineti/test/tmp_test.py b/netineti/test/tmp_test.py
index 7c46c98..9b7b7a1 100755
--- a/netineti/test/tmp_test.py
+++ b/netineti/test/tmp_test.py
@@ -1,11 +1,12 @@
 #!/usr/bin/env python
-import subprocess
-import shlex
+""" Checks if new code creates signfificantly different results with old code.
+Note, that significant difference might be desirable --- this test allows to
+find out which changes exactly happened."""
 import math
 import sys
 import time
-import os
 import difflib
+import pickle
 from netineti.trainer import NetiNetiTrainer
 from netineti.finder import NetiNeti
 
@@ -13,7 +14,7 @@ num_cycles = 3
 
 if len(sys.argv) > 1:
     try:
-        num_cycles =  int(sys.argv[1]) + 1
+        num_cycles = int(sys.argv[1]) + 1
     except ValueError:
         pass
 
@@ -51,6 +52,7 @@ population = []
 
 time_start = time.clock()
 classifier = NetiNetiTrainer(learning_algorithm='NB')
+pickle.dump(classifier, open('../data/netineti_trainer_dump', 'wb'))
 time_training = time.clock()
 print "Training time: %s" % (time_training - time_start)
 nn = NetiNeti(classifier)
